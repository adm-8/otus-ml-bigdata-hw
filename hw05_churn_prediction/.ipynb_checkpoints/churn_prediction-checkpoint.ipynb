{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "// import $ivy.`sh.almond::almond-spark:0.10.9` \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.4` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.4` // Or use any other 2.x version here\n",
    "// import $ivy.`sh.almond::almond-spark:0.10.9` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@194f9b39\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.testing.memory\", 471859200)\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, instanceId_objectType: string ... 167 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = spark.read.format(\"parquet\").load(\"data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"trainDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сформировать критерии оттекших/удержавшихся пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instanceId_userId\n",
      "instanceId_objectType\n",
      "instanceId_objectId\n",
      "audit_pos\n",
      "audit_clientType\n",
      "audit_timestamp\n",
      "audit_timePassed\n",
      "audit_experiment\n",
      "audit_resourceType\n",
      "metadata_ownerId\n",
      "metadata_ownerType\n",
      "metadata_createdAt\n",
      "metadata_authorId\n",
      "metadata_applicationId\n",
      "metadata_numCompanions\n",
      "metadata_numPhotos\n",
      "metadata_numPolls\n",
      "metadata_numSymbols\n",
      "metadata_numTokens\n",
      "metadata_numVideos\n",
      "metadata_platform\n",
      "metadata_totalVideoLength\n",
      "metadata_options\n",
      "relationsMask\n",
      "userOwnerCounters_USER_FEED_REMOVE\n",
      "userOwnerCounters_USER_PROFILE_VIEW\n",
      "userOwnerCounters_VOTE_POLL\n",
      "userOwnerCounters_USER_SEND_MESSAGE\n",
      "userOwnerCounters_USER_DELETE_MESSAGE\n",
      "userOwnerCounters_USER_INTERNAL_LIKE\n",
      "userOwnerCounters_USER_INTERNAL_UNLIKE\n",
      "userOwnerCounters_USER_STATUS_COMMENT_CREATE\n",
      "userOwnerCounters_PHOTO_COMMENT_CREATE\n",
      "userOwnerCounters_MOVIE_COMMENT_CREATE\n",
      "userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE\n",
      "userOwnerCounters_COMMENT_INTERNAL_LIKE\n",
      "userOwnerCounters_USER_FORUM_MESSAGE_CREATE\n",
      "userOwnerCounters_PHOTO_MARK_CREATE\n",
      "userOwnerCounters_PHOTO_VIEW\n",
      "userOwnerCounters_PHOTO_PIN_BATCH_CREATE\n",
      "userOwnerCounters_PHOTO_PIN_UPDATE\n",
      "userOwnerCounters_USER_PRESENT_SEND\n",
      "userOwnerCounters_UNKNOWN\n",
      "userOwnerCounters_CREATE_TOPIC\n",
      "userOwnerCounters_CREATE_IMAGE\n",
      "userOwnerCounters_CREATE_MOVIE\n",
      "userOwnerCounters_CREATE_COMMENT\n",
      "userOwnerCounters_CREATE_LIKE\n",
      "userOwnerCounters_TEXT\n",
      "userOwnerCounters_IMAGE\n",
      "userOwnerCounters_VIDEO\n",
      "ownerUserCounters_USER_FEED_REMOVE\n",
      "ownerUserCounters_USER_PROFILE_VIEW\n",
      "ownerUserCounters_VOTE_POLL\n",
      "ownerUserCounters_USER_SEND_MESSAGE\n",
      "ownerUserCounters_USER_DELETE_MESSAGE\n",
      "ownerUserCounters_USER_INTERNAL_LIKE\n",
      "ownerUserCounters_USER_INTERNAL_UNLIKE\n",
      "ownerUserCounters_USER_STATUS_COMMENT_CREATE\n",
      "ownerUserCounters_PHOTO_COMMENT_CREATE\n",
      "ownerUserCounters_MOVIE_COMMENT_CREATE\n",
      "ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE\n",
      "ownerUserCounters_COMMENT_INTERNAL_LIKE\n",
      "ownerUserCounters_USER_FORUM_MESSAGE_CREATE\n",
      "ownerUserCounters_PHOTO_MARK_CREATE\n",
      "ownerUserCounters_PHOTO_VIEW\n",
      "ownerUserCounters_PHOTO_PIN_BATCH_CREATE\n",
      "ownerUserCounters_PHOTO_PIN_UPDATE\n",
      "ownerUserCounters_USER_PRESENT_SEND\n",
      "ownerUserCounters_UNKNOWN\n",
      "ownerUserCounters_CREATE_TOPIC\n",
      "ownerUserCounters_CREATE_IMAGE\n",
      "ownerUserCounters_CREATE_MOVIE\n",
      "ownerUserCounters_CREATE_COMMENT\n",
      "ownerUserCounters_CREATE_LIKE\n",
      "ownerUserCounters_TEXT\n",
      "ownerUserCounters_IMAGE\n",
      "ownerUserCounters_VIDEO\n",
      "membership_status\n",
      "membership_statusUpdateDate\n",
      "membership_joinDate\n",
      "membership_joinRequestDate\n",
      "owner_create_date\n",
      "owner_birth_date\n",
      "owner_gender\n",
      "owner_status\n",
      "owner_ID_country\n",
      "owner_ID_Location\n",
      "owner_is_active\n",
      "owner_is_deleted\n",
      "owner_is_abused\n",
      "owner_is_activated\n",
      "owner_change_datime\n",
      "owner_is_semiactivated\n",
      "owner_region\n",
      "user_create_date\n",
      "user_birth_date\n",
      "user_gender\n",
      "user_status\n",
      "user_ID_country\n",
      "user_ID_Location\n",
      "user_is_active\n",
      "user_is_deleted\n",
      "user_is_abused\n",
      "user_is_activated\n",
      "user_change_datime\n",
      "user_is_semiactivated\n",
      "user_region\n",
      "feedback\n",
      "objectId\n",
      "auditweights_ageMs\n",
      "auditweights_closed\n",
      "auditweights_ctr_gender\n",
      "auditweights_ctr_high\n",
      "auditweights_ctr_negative\n",
      "auditweights_dailyRecency\n",
      "auditweights_feedOwner_RECOMMENDED_GROUP\n",
      "auditweights_feedStats\n",
      "auditweights_friendCommentFeeds\n",
      "auditweights_friendCommenters\n",
      "auditweights_friendLikes\n",
      "auditweights_friendLikes_actors\n",
      "auditweights_hasDetectedText\n",
      "auditweights_hasText\n",
      "auditweights_isPymk\n",
      "auditweights_isRandom\n",
      "auditweights_likersFeedStats_hyper\n",
      "auditweights_likersSvd_prelaunch_hyper\n",
      "auditweights_matrix\n",
      "auditweights_notOriginalPhoto\n",
      "auditweights_numDislikes\n",
      "auditweights_numLikes\n",
      "auditweights_numShows\n",
      "auditweights_onlineVideo\n",
      "auditweights_partAge\n",
      "auditweights_partCtr\n",
      "auditweights_partSvd\n",
      "auditweights_processedVideo\n",
      "auditweights_relationMasks\n",
      "auditweights_source_LIVE_TOP\n",
      "auditweights_source_MOVIE_TOP\n",
      "auditweights_svd_prelaunch\n",
      "auditweights_svd_spark\n",
      "auditweights_userAge\n",
      "auditweights_userOwner_CREATE_COMMENT\n",
      "auditweights_userOwner_CREATE_IMAGE\n",
      "auditweights_userOwner_CREATE_LIKE\n",
      "auditweights_userOwner_IMAGE\n",
      "auditweights_userOwner_MOVIE_COMMENT_CREATE\n",
      "auditweights_userOwner_PHOTO_COMMENT_CREATE\n",
      "auditweights_userOwner_PHOTO_MARK_CREATE\n",
      "auditweights_userOwner_PHOTO_VIEW\n",
      "auditweights_userOwner_TEXT\n",
      "auditweights_userOwner_UNKNOWN\n",
      "auditweights_userOwner_USER_DELETE_MESSAGE\n",
      "auditweights_userOwner_USER_FEED_REMOVE\n",
      "auditweights_userOwner_USER_FORUM_MESSAGE_CREATE\n",
      "auditweights_userOwner_USER_INTERNAL_LIKE\n",
      "auditweights_userOwner_USER_INTERNAL_UNLIKE\n",
      "auditweights_userOwner_USER_PRESENT_SEND\n",
      "auditweights_userOwner_USER_PROFILE_VIEW\n",
      "auditweights_userOwner_USER_SEND_MESSAGE\n",
      "auditweights_userOwner_USER_STATUS_COMMENT_CREATE\n",
      "auditweights_userOwner_VIDEO\n",
      "auditweights_userOwner_VOTE_POLL\n",
      "auditweights_x_ActorsRelations\n",
      "auditweights_likersSvd_spark_hyper\n",
      "auditweights_source_PROMO\n",
      "date\n"
     ]
    }
   ],
   "source": [
    "// посмотрим на список колонок\n",
    "for (col <- df.columns){\n",
    "    println(col)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------+\n",
      "| min(date)| max(date)|datediff(max(date), min(date))|\n",
      "+----------+----------+------------------------------+\n",
      "|2018-02-01|2018-03-21|                            48|\n",
      "+----------+----------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// посмотрим минимальное, максимальное значение даты и разницу между ними\n",
    "spark.sql(\"\"\"\n",
    "    select min(date), max(date), datediff(max(date), min(date))\n",
    "    from trainDF\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### будем считать, что если пользователь не совершал никаких действий более трех недель, то он уже и не вернется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_tgt\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, instanceId_objectType: string ... 168 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_tgt = spark.sql(\"\"\"\n",
    "with cte_last_action as (\n",
    "    select instanceId_userId, max(date) as md from trainDF group by instanceId_userId\n",
    ")\n",
    ", cte_target as (\n",
    "    select \n",
    "        instanceId_userId\n",
    "        , case when datediff('2018-03-21', md) > 21 then 1 else 0 end as target\n",
    "    from cte_last_action\n",
    ") select e.*, t.target from trainDF e\n",
    "    join cte_target t on e.instanceId_userId = t.instanceId_userId\n",
    "\"\"\")\n",
    "\n",
    "df_tgt.createOrReplaceTempView(\"df_tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|target|count(1)|\n",
      "+------+--------+\n",
      "|     1| 1925038|\n",
      "|     0|16361537|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// посмотрим сколько у нас получилось оттёкших пользователей\n",
    "spark.sql(\"\"\"\n",
    "    select target, count(*) from df_tgt group by target\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовить признаки, обучить модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдвинем гипотезу, что отток напрямую зависит либо от общего кол-ва взаимодействий с пабликами в течение последних Х недель, или же от каких-то конкретных взаимодействий в течение последних Х недель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_exp_tgt\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, fb_exp: string ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// расширим наш врейм, вытащив список реакций в каждую отдельную строку\n",
    "\n",
    "var df_exp_tgt = spark.sql(\"\"\"\n",
    "    select \n",
    "        instanceId_userId\n",
    "        , explode(feedback) as fb_exp \n",
    "        , date\n",
    "        , target\n",
    "    from df_tgt t \n",
    "\"\"\")\n",
    "df_exp_tgt.createOrReplaceTempView(\"df_exp_tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_bucket\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, fb_exp: string ... 2 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// разобьем разницу между датой действия и последней даты на группы с шагом 7\n",
    "// , чтобы на выходе можно было посчитать кол-во действий в ретроспективе\n",
    "// , сгруппировав их по этим \"лагам времени\" и типам реакций\n",
    "\n",
    "val df_bucket = spark.sql(\"\"\"\n",
    "    select \n",
    "        instanceId_userId\n",
    "        , explode(feedback) as fb_exp \n",
    "        , floor(datediff('2018-03-21',date) / 7) date_diff_bucket\n",
    "        , target\n",
    "    from df_tgt t \n",
    "    order by date\n",
    "\"\"\")\n",
    "df_bucket.createOrReplaceTempView(\"df_bucket\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_act_type_lag\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, act_type: string ... 1 more field]\n",
       "\u001b[36mdf_pivot_act_type\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 62 more fields]\n",
       "\u001b[36mdf_lag\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, total_lag: string ... 1 more field]\n",
       "\u001b[36mdf_pivot_lag\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, total_0: bigint ... 6 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// сделаем фрейм с разбивкой по типу действия + недельному лагу\n",
    "val df_act_type_lag = spark.sql(\"\"\"\n",
    "select \n",
    "    instanceId_userId\n",
    "    , concat(fb_exp, '_', date_diff_bucket) as act_type\n",
    "    , target\n",
    "from df_bucket\n",
    "\"\"\")\n",
    "\n",
    "val df_pivot_act_type = df_act_type_lag\n",
    "    .groupBy(\"instanceId_userId\")\n",
    "    .pivot(\"act_type\")\n",
    "    .count()\n",
    "    .na.fill(0)\n",
    "\n",
    "df_pivot_act_type.createOrReplaceTempView(\"df_pivot_act_type\")\n",
    "\n",
    "// сделаем фрейм с разбивкой по недельному лагу\n",
    "val df_lag = spark.sql(\"\"\"\n",
    "select \n",
    "    instanceId_userId\n",
    "    , concat('total_', date_diff_bucket) as total_lag\n",
    "    , target\n",
    "from df_bucket\n",
    "\"\"\")\n",
    "\n",
    "val df_pivot_lag = df_lag\n",
    "    .groupBy(\"instanceId_userId\")\n",
    "    .pivot(\"total_lag\")\n",
    "    .count()\n",
    "    .na.fill(0)\n",
    "\n",
    "df_pivot_lag.createOrReplaceTempView(\"df_pivot_lag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_for_split\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 70 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_for_split = spark.sql(\"\"\"\n",
    "    select \n",
    "        pa.*\n",
    "        , la.total_0\n",
    "        , la.total_1\n",
    "        , la.total_2\n",
    "        , la.total_3\n",
    "        , la.total_4\n",
    "        , la.total_5\n",
    "        , la.total_6\n",
    "        , t.target\n",
    "    from df_tgt t\n",
    "    left join df_pivot_act_type pa on t.instanceId_userId = pa.instanceId_userId\n",
    "    left join df_pivot_lag la on t.instanceId_userId = la.instanceId_userId\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, получили разряженную матрицу с кол-ом реакций, разбитых по неделям и по типам + неделям. Теперь выборку можно разбить на трейн и тест, обучить модель и посмотреть что из этого вышло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.VectorAssembler\n",
       "\n",
       "// получим массив колонок с фичами\n",
       "\u001b[39m\n",
       "\u001b[36mf_arr\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"Clicked_1\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_2\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_3\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_4\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_5\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_6\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_1\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_2\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_3\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_4\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_5\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_6\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_1\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_2\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_3\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_4\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_5\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_6\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_1\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_2\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_3\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_4\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_5\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_6\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_1\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_2\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_3\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_4\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_5\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_6\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_1\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_2\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_3\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_4\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_5\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_6\"\u001b[39m,\n",
       "  \u001b[32m\"ReShared_1\"\u001b[39m,\n",
       "  \u001b[32m\"ReShared_2\"\u001b[39m,\n",
       "...\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_3f6770eb7024\n",
       "\u001b[36mdf_features\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 71 more fields]\n",
       "\u001b[36mtrainDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [instanceId_userId: int, Clicked_0: bigint ... 71 more fields]\n",
       "\u001b[36mtestDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [instanceId_userId: int, Clicked_0: bigint ... 71 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// для начала нам необходимо объединить все фичи в один вектор, чтобы потом скормить его модели\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "// получим массив колонок с фичами\n",
    "var f_arr = Array[String]()\n",
    "for (col <- df_for_split.columns){\n",
    "    if(col != \"instanceId_userId\" && col != \"target\" && col.endsWith(\"_0\") != true)\n",
    "        // срезали последнюю неделю, чтобы моделе было интереснее =)\n",
    "        f_arr = f_arr :+ col\n",
    "}\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(f_arr)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val df_features = assembler.transform(df_for_split)\n",
    "\n",
    "// разделим набор данных на трейн и тест\n",
    "val Array(trainDF, testDF) = df_features.randomSplit(Array(0.7, 0.3), seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "// Train a GBT model.\n",
       "\u001b[39m\n",
       "\u001b[36mgbt\u001b[39m: \u001b[32mGBTClassifier\u001b[39m = gbtc_cec363ded67e\n",
       "\u001b[36mgbtModel\u001b[39m: \u001b[32mGBTClassificationModel\u001b[39m = GBTClassificationModel (uid=gbtc_cec363ded67e) with 10 trees"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Попробуем обучить модель градиентного бустинга\n",
    "\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"target\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setMaxIter(10)\n",
    "\n",
    "val gbtModel = gbt.fit(trainDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|target|prediction|count(1)|\n",
      "+------+----------+--------+\n",
      "|     1|       0.0|   97997|\n",
      "|     0|       0.0| 4651170|\n",
      "|     1|       1.0|  480112|\n",
      "|     0|       1.0|  255063|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_gbt_pred\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 74 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_gbt_pred = gbtModel.transform(testDF)\n",
    "df_gbt_pred.createOrReplaceTempView(\"df_gbt_pred\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select target, prediction, count(*)\n",
    "    from df_gbt_pred\n",
    "    group by target, prediction\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mTP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m480112.0\u001b[39m\n",
       "\u001b[36mTN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m4651170.0\u001b[39m\n",
       "\u001b[36mFP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m255063.0\u001b[39m\n",
       "\u001b[36mFN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m97997.0\u001b[39m\n",
       "\u001b[36mPPV\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.6530581154146972\u001b[39m\n",
       "\u001b[36mTPR\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.83048698428843\u001b[39m\n",
       "\u001b[36mF1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.7311624903676585\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// посчитаем F1Score для модели бустинга\n",
    "val TP = 480112.0\n",
    "val TN = 4651170.0\n",
    "val FP = 255063.0\n",
    "val FN = 97997.0\n",
    "\n",
    "val PPV = TP / (TP + FP) // Precision (Positive Predictive Value)\n",
    "val TPR = TP / (TP + FN) // Recall (True Positive Rate)\n",
    "val F1 = (2 * PPV * TPR) / (PPV + TPR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.NaiveBayes\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mnbModel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mNaiveBayesModel\u001b[39m = NaiveBayesModel (uid=nb_09faf60a372e) with 2 classes"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Попробуем обучить модель наивного байеса\n",
    "\n",
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "\n",
    "val nbModel = new NaiveBayes()\n",
    "    .setLabelCol(\"target\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|target|prediction|count(1)|\n",
      "+------+----------+--------+\n",
      "|     1|       0.0|  242299|\n",
      "|     0|       0.0| 4670866|\n",
      "|     1|       1.0|  335810|\n",
      "|     0|       1.0|  235367|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_nb_pred\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 74 more fields]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_nb_pred = nbModel.transform(testDF)\n",
    "df_nb_pred.createOrReplaceTempView(\"df_nb_pred\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select target, prediction, count(*)\n",
    "    from df_nb_pred\n",
    "    group by target, prediction\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mTP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m335810.0\u001b[39m\n",
       "\u001b[36mTN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m4670866.0\u001b[39m\n",
       "\u001b[36mFP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m255063.0\u001b[39m\n",
       "\u001b[36mFN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m235367.0\u001b[39m\n",
       "\u001b[36mPPV\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.5683285579134603\u001b[39m\n",
       "\u001b[36mTPR\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.587926334568794\u001b[39m\n",
       "\u001b[36mF1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.5779613613872037\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// посчитаем F1Score для модели бустинга\n",
    "val TP = 335810.0\n",
    "val TN = 4670866.0\n",
    "val FP = 255063.0\n",
    "val FN = 235367.0\n",
    "\n",
    "val PPV = TP / (TP + FP) // Precision (Positive Predictive Value)\n",
    "val TPR = TP / (TP + FN) // Recall (True Positive Rate)\n",
    "val F1 = (2 * PPV * TPR) / (PPV + TPR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показазатели конечно не фонтан, но будем считать, что нас это устраивает т.к. по заданию перед нами не стоит цель получить максимально точную модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Интерпретируем модель, сделать сегментацию пользователей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь вспоминаем, что с интерпретацией моделей в скале всё очень плохо, страдаем какое-то время. Вспоминаем, что есть PravdaML и пробуем запилить модель, используюя PravdaML, чтобы можно было интрпретировать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "// import org.apache.spark.ml.odkl.MatrixLBFGS\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.odkl.XGBoostClassifier\u001b[39m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`ru.odnoklassniki::pravda-ml:0.6.2`\n",
    "// import org.apache.spark.ml.odkl.MatrixLBFGS\n",
    "import org.apache.spark.ml.classification.odkl.XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pmlXGBModel = new XGBoostClassifier()\n",
    "    .setLabelCol(\"target\")\n",
    "    .fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
