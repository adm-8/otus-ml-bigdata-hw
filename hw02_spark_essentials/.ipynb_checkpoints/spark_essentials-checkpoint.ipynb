{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.4` // Or use any other 2.x version here\n",
    "// import $ivy.`sh.almond::almond-spark:0.10.9` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.testing.memory\", 471859200)\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var df = spark.read.format(\"parquet\").load(\"data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//  посмотрим на список колонок\n",
    "/*\n",
    "for (c <- df.columns) {\n",
    "    println(c)\n",
    "}\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"trainDF\")\n",
    "var df_exp = spark.sql(\"\"\"\n",
    "    select t.*, explode(feedback) as fb_exp from trainDF t \n",
    "\"\"\")\n",
    "df_exp.createOrReplaceTempView(\"trainDF_exp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1 - Постройте топ популярных групп на портале"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select metadata_ownerId, count(*) as like_cnt from trainDF_exp\n",
    "    where fb_exp = 'Liked'\n",
    "    group by metadata_ownerId\n",
    "    order by like_cnt desc\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2 - Построите гистограммы популярности/активности групп на портале по времени суток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select distinct fb_exp from trainDF_exp\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var activityDF = spark.sql(\"\"\"\n",
    "    select created_hour as created_hour_a, count(*) as cnt_a from (\n",
    "        select \n",
    "            from_unixtime(metadata_createdAt, 'HH') as created_hour\n",
    "            , fb_exp\n",
    "        from trainDF_exp \n",
    "        where fb_exp <> 'Ignored'\n",
    "    ) \n",
    "    group by created_hour\n",
    "\"\"\").as(\"df_activity\")\n",
    "\n",
    "var popularityDF = spark.sql(\"\"\"\n",
    "    select created_hour as created_hour_p, count(*) as cnt_p from (\n",
    "        select \n",
    "            from_unixtime(metadata_createdAt, 'HH') as created_hour\n",
    "            , fb_exp\n",
    "        from trainDF_exp \n",
    "        where fb_exp = 'Liked'\n",
    "    ) \n",
    "    group by created_hour\n",
    "\"\"\").as(\"df_popularity\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var joined_df = activityDF.join(\n",
    "popularityDF\n",
    ", $\"created_hour_a\" === $\"created_hour_p\"\n",
    ", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show(24)\n",
    "joined_df.createOrReplaceTempView(\"joined_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.0`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var j_res = spark.sql(\"\"\"\n",
    "    select created_hour_a, cnt_a, cnt_p\n",
    "    from joined_df\n",
    "    order by created_hour_a\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "var hours = Seq[String]()\n",
    "var cnt_a = Seq[Int]()\n",
    "var cnt_p = Seq[Int]()\n",
    "\n",
    "for (row <- j_res.collect()){\n",
    "    hours = hours :+ row(0).asInstanceOf[String] + 'h'\n",
    "    cnt_a = cnt_a :+ row(1).asInstanceOf[Long].toInt\n",
    "    cnt_p = cnt_p :+ row(2).asInstanceOf[Long].toInt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trace1 = Bar(\n",
    "  hours,\n",
    "  cnt_a,\n",
    "  \"Активность\",\n",
    "  text = hours.map(x => \"Час \" + x),\n",
    "  // orientation = Orientation.Horizontal,\n",
    "  marker = Marker(\n",
    "    color = Color.RGB(49, 130, 189),\n",
    "    opacity = 0.7))\n",
    "\n",
    "val trace2 = Bar(\n",
    "  hours,\n",
    "  cnt_p,\n",
    "  \"Популярность\",\n",
    "  text = hours.map(x => \"Час \" + x),\n",
    "  // orientation = Orientation.Horizontal,\n",
    "  marker = Marker(\n",
    "    color = Color.RGB(204, 0, 0),\n",
    "    opacity = 0.7))\n",
    "\n",
    "Seq(trace1, trace2).plot(\"Данные активности и популярности групп по часам\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// обнулим переменные\n",
    "activityDF = null\n",
    "popularityDF = null\n",
    "joined_df = null\n",
    "j_res = null\n",
    "df = null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посольку груфик не отображается в случае, если открыть ноутбук напрямую из гитхаба, продублирую его в виде картинки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"activity_popularity.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3 - Посчитайте корреляцию признаков с целевой переменной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### посмотрим какие типы вообще есть в нашем наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var fieldTypes = Seq[String]()\n",
    "val df_exp_int = \n",
    "for ( field <- df_exp.dtypes) {\n",
    "    fieldTypes = fieldTypes :+ field._2\n",
    "}\n",
    "fieldTypes.distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var numColumns = Seq[String]()\n",
    "var strColumns = Seq[String]()\n",
    "var dateColumns = Seq[String]()\n",
    "\n",
    "for ( field <- df_exp.dtypes ) {\n",
    "    if(field._2 == \"StringType\")\n",
    "        strColumns = strColumns :+ field._1\n",
    "    else if(field._2 == \"IntegerType\" || field._2 == \"DoubleType\" || field._2 == \"LongType\")\n",
    "        numColumns = numColumns :+ field._1\n",
    "    else if(field._2 == \"DateType\")\n",
    "        dateColumns = dateColumns :+ field._1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поскольку в задании нет четкого определения целевой переменной, будем считать, что ей является признак того, поставили лайк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// и доавим день недели \n",
    "var trainDF_exp = spark.sql(\"\"\"\n",
    "    select t.*\n",
    "    , date_format(date, 'u') as week_day_number\n",
    "    , case when t.fb_exp = 'Liked' then 1 else 0 end as target\n",
    "    from trainDF_exp t\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-mllib:2.4.4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Напишем функцию, которая будет применять StringIndexer к фрейму и обогощать его новой колонкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "def customStringIndexer(df:DataFrame, columnName:String, columnNameIx:String) : DataFrame = {\n",
    "    val indexer = new StringIndexer()\n",
    "    .setInputCol(columnName)\n",
    "    .setOutputCol(columnNameIx) \n",
    "    \n",
    "    println(\"Indexing \" + columnName + \"...\")\n",
    "    \n",
    "    return indexer.fit(df).transform(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применим функцию ко всем строковым колонкам, кроме той, на основе которой мы делали целевую переменную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (strColName <- strColumns) {\n",
    "    // с membership_status какая-то пролема, пока не было времени разобраться, пока просто исключил её \n",
    "    if(strColName != \"fb_exp\" && strColName != \"membership_status\"){ \n",
    "        trainDF_exp = customStringIndexer(trainDF_exp, strColName, strColName + \"_ix\")\n",
    "        numColumns = numColumns :+ strColName + \"_ix\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// попробуем выбрать новые колоноки, посмотрим что получилось:\n",
    "trainDF_exp.select(\n",
    "    \"instanceId_objectType_ix\",\n",
    "    \"audit_clientType_ix\",\n",
    "    \"audit_experiment_ix\",\n",
    "    \"metadata_ownerType_ix\",\n",
    "    \"metadata_platform_ix\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем функцию, которая посчитает нам корреляцию признаков с возможностью получить имена колонок, которые коррелируют с целевой переменной на более чем minToReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getCorr (df:DataFrame, columnNames:Seq[String], target:String, minToReturn:Double) : Seq[String] = {\n",
    "    var ret = Seq[String]()\n",
    "    for (col <- columnNames){\n",
    "        val corr = trainDF_exp.stat.corr(col, target)  \n",
    "        println(target + \" | \" + col + \" \\t \" + corr)  \n",
    "        if(corr.abs >= minToReturn)\n",
    "            ret = ret :+ col\n",
    "    }\n",
    "    return ret\n",
    "}\n",
    "\n",
    "\n",
    "val corrColumns = getCorr(\n",
    "    trainDF_exp\n",
    "    ,numColumns\n",
    "    , \"target\"\n",
    "    , 0.05)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// псоморим на список колонок, которые корелируют с целевой переменной более чем на 0.05  \n",
    "corrColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подведем итоги\n",
    "\n",
    "* Больше всего лайков собрали посты групп с идентификаторами 37463, 11222, 18942\n",
    "* Активность и популяроность не сильно зависит от вреени дня\n",
    "* Судя по предварительному анализу крайне мало признаков коррелируют с целевой переменной. Но это не означает, что её вовсе нет, она может быть и неявной.\n",
    "\n",
    "* Категориальные переменные в идеале надо было бы прогнать через One Hot Encoding, быть может лайки зависят от конкретных значений в той или иной категориальной колонки, но для ДЗ прикручивать OHE уже не усспеваю т.к. и так на домашку ушло уйму времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## полезные ссылочки на тему:\n",
    "* конвертируем категориальные признаки в числовые https://spark.apache.org/docs/2.4.4/ml-features.html#stringindexer\n",
    "* применяем OHE https://spark.apache.org/docs/2.4.4/ml-features.html#onehotencoderestimator\n",
    "* векторизируем https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "* нармализуем данные, например при помощи StandardScaller https://spark.apache.org/docs/2.4.4/ml-features.html#standardscaler\n",
    "* считаем корреляцию https://spark.apache.org/docs/2.4.4/ml-statistics.html#correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.MinMaxScaler\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "\n",
    "val colName = \"metadata_applicationId\"\n",
    "val targetColName = \"target\"\n",
    "\n",
    "println(\"Getting columns \" + colName + \", \" + targetColName + \"...\")\n",
    "val curColDF = trainDF_exp.select(colName, targetColName).na.fill(0.0)\n",
    "\n",
    "// векторизуем признаки т.к. для нормализации нужен именно вектор\n",
    "println(\"Vectorizing \" + colName + \"...\")\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(colName, targetColName))\n",
    "  .setOutputCol(\"result\")\n",
    "val assembledDF = assembler.transform(curColDF)\n",
    "\n",
    "// нормализуем данные\n",
    "println(\"Data normalization ...\")\n",
    "val scaler = new MinMaxScaler()\n",
    "  .setInputCol(\"result\")\n",
    "  .setOutputCol(\"result_scaled\")\n",
    "\n",
    "println(\"Fitting scaler model...\")\n",
    "val scalerModel = scaler.fit(assembledDF)\n",
    "\n",
    "println(\"Transforming data...\")\n",
    "val scaledDF = scalerModel.transform(assembledDF)\n",
    "\n",
    "// Получаем коэффициент корреляции\n",
    "println(\"Calculating correlation between \" + colName + \" and \" + targetColName + \" columns...\")\n",
    "val Row(coeff1: Matrix) = Correlation.corr(scaledDF, \"result_scaled\").head\n",
    "val corrValue = coeff1(0,1)\n",
    "println(targetColName + \" | \" + colName + \" \\t \" + corrValue)  \n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
