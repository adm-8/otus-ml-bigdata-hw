{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "// import $ivy.`sh.almond::almond-spark:0.10.9` \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.4` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.4` // Or use any other 2.x version here\n",
    "// import $ivy.`sh.almond::almond-spark:0.10.9` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@194f9b39\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.testing.memory\", 471859200)\n",
    "    .config(\"spark.executor.instances\", \"4\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, instanceId_objectType: string ... 167 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = spark.read.format(\"parquet\").load(\"data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"trainDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сформировать критерии оттекших/удержавшихся пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instanceId_userId\n",
      "instanceId_objectType\n",
      "instanceId_objectId\n",
      "audit_pos\n",
      "audit_clientType\n",
      "audit_timestamp\n",
      "audit_timePassed\n",
      "audit_experiment\n",
      "audit_resourceType\n",
      "metadata_ownerId\n",
      "metadata_ownerType\n",
      "metadata_createdAt\n",
      "metadata_authorId\n",
      "metadata_applicationId\n",
      "metadata_numCompanions\n",
      "metadata_numPhotos\n",
      "metadata_numPolls\n",
      "metadata_numSymbols\n",
      "metadata_numTokens\n",
      "metadata_numVideos\n",
      "metadata_platform\n",
      "metadata_totalVideoLength\n",
      "metadata_options\n",
      "relationsMask\n",
      "userOwnerCounters_USER_FEED_REMOVE\n",
      "userOwnerCounters_USER_PROFILE_VIEW\n",
      "userOwnerCounters_VOTE_POLL\n",
      "userOwnerCounters_USER_SEND_MESSAGE\n",
      "userOwnerCounters_USER_DELETE_MESSAGE\n",
      "userOwnerCounters_USER_INTERNAL_LIKE\n",
      "userOwnerCounters_USER_INTERNAL_UNLIKE\n",
      "userOwnerCounters_USER_STATUS_COMMENT_CREATE\n",
      "userOwnerCounters_PHOTO_COMMENT_CREATE\n",
      "userOwnerCounters_MOVIE_COMMENT_CREATE\n",
      "userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE\n",
      "userOwnerCounters_COMMENT_INTERNAL_LIKE\n",
      "userOwnerCounters_USER_FORUM_MESSAGE_CREATE\n",
      "userOwnerCounters_PHOTO_MARK_CREATE\n",
      "userOwnerCounters_PHOTO_VIEW\n",
      "userOwnerCounters_PHOTO_PIN_BATCH_CREATE\n",
      "userOwnerCounters_PHOTO_PIN_UPDATE\n",
      "userOwnerCounters_USER_PRESENT_SEND\n",
      "userOwnerCounters_UNKNOWN\n",
      "userOwnerCounters_CREATE_TOPIC\n",
      "userOwnerCounters_CREATE_IMAGE\n",
      "userOwnerCounters_CREATE_MOVIE\n",
      "userOwnerCounters_CREATE_COMMENT\n",
      "userOwnerCounters_CREATE_LIKE\n",
      "userOwnerCounters_TEXT\n",
      "userOwnerCounters_IMAGE\n",
      "userOwnerCounters_VIDEO\n",
      "ownerUserCounters_USER_FEED_REMOVE\n",
      "ownerUserCounters_USER_PROFILE_VIEW\n",
      "ownerUserCounters_VOTE_POLL\n",
      "ownerUserCounters_USER_SEND_MESSAGE\n",
      "ownerUserCounters_USER_DELETE_MESSAGE\n",
      "ownerUserCounters_USER_INTERNAL_LIKE\n",
      "ownerUserCounters_USER_INTERNAL_UNLIKE\n",
      "ownerUserCounters_USER_STATUS_COMMENT_CREATE\n",
      "ownerUserCounters_PHOTO_COMMENT_CREATE\n",
      "ownerUserCounters_MOVIE_COMMENT_CREATE\n",
      "ownerUserCounters_USER_PHOTO_ALBUM_COMMENT_CREATE\n",
      "ownerUserCounters_COMMENT_INTERNAL_LIKE\n",
      "ownerUserCounters_USER_FORUM_MESSAGE_CREATE\n",
      "ownerUserCounters_PHOTO_MARK_CREATE\n",
      "ownerUserCounters_PHOTO_VIEW\n",
      "ownerUserCounters_PHOTO_PIN_BATCH_CREATE\n",
      "ownerUserCounters_PHOTO_PIN_UPDATE\n",
      "ownerUserCounters_USER_PRESENT_SEND\n",
      "ownerUserCounters_UNKNOWN\n",
      "ownerUserCounters_CREATE_TOPIC\n",
      "ownerUserCounters_CREATE_IMAGE\n",
      "ownerUserCounters_CREATE_MOVIE\n",
      "ownerUserCounters_CREATE_COMMENT\n",
      "ownerUserCounters_CREATE_LIKE\n",
      "ownerUserCounters_TEXT\n",
      "ownerUserCounters_IMAGE\n",
      "ownerUserCounters_VIDEO\n",
      "membership_status\n",
      "membership_statusUpdateDate\n",
      "membership_joinDate\n",
      "membership_joinRequestDate\n",
      "owner_create_date\n",
      "owner_birth_date\n",
      "owner_gender\n",
      "owner_status\n",
      "owner_ID_country\n",
      "owner_ID_Location\n",
      "owner_is_active\n",
      "owner_is_deleted\n",
      "owner_is_abused\n",
      "owner_is_activated\n",
      "owner_change_datime\n",
      "owner_is_semiactivated\n",
      "owner_region\n",
      "user_create_date\n",
      "user_birth_date\n",
      "user_gender\n",
      "user_status\n",
      "user_ID_country\n",
      "user_ID_Location\n",
      "user_is_active\n",
      "user_is_deleted\n",
      "user_is_abused\n",
      "user_is_activated\n",
      "user_change_datime\n",
      "user_is_semiactivated\n",
      "user_region\n",
      "feedback\n",
      "objectId\n",
      "auditweights_ageMs\n",
      "auditweights_closed\n",
      "auditweights_ctr_gender\n",
      "auditweights_ctr_high\n",
      "auditweights_ctr_negative\n",
      "auditweights_dailyRecency\n",
      "auditweights_feedOwner_RECOMMENDED_GROUP\n",
      "auditweights_feedStats\n",
      "auditweights_friendCommentFeeds\n",
      "auditweights_friendCommenters\n",
      "auditweights_friendLikes\n",
      "auditweights_friendLikes_actors\n",
      "auditweights_hasDetectedText\n",
      "auditweights_hasText\n",
      "auditweights_isPymk\n",
      "auditweights_isRandom\n",
      "auditweights_likersFeedStats_hyper\n",
      "auditweights_likersSvd_prelaunch_hyper\n",
      "auditweights_matrix\n",
      "auditweights_notOriginalPhoto\n",
      "auditweights_numDislikes\n",
      "auditweights_numLikes\n",
      "auditweights_numShows\n",
      "auditweights_onlineVideo\n",
      "auditweights_partAge\n",
      "auditweights_partCtr\n",
      "auditweights_partSvd\n",
      "auditweights_processedVideo\n",
      "auditweights_relationMasks\n",
      "auditweights_source_LIVE_TOP\n",
      "auditweights_source_MOVIE_TOP\n",
      "auditweights_svd_prelaunch\n",
      "auditweights_svd_spark\n",
      "auditweights_userAge\n",
      "auditweights_userOwner_CREATE_COMMENT\n",
      "auditweights_userOwner_CREATE_IMAGE\n",
      "auditweights_userOwner_CREATE_LIKE\n",
      "auditweights_userOwner_IMAGE\n",
      "auditweights_userOwner_MOVIE_COMMENT_CREATE\n",
      "auditweights_userOwner_PHOTO_COMMENT_CREATE\n",
      "auditweights_userOwner_PHOTO_MARK_CREATE\n",
      "auditweights_userOwner_PHOTO_VIEW\n",
      "auditweights_userOwner_TEXT\n",
      "auditweights_userOwner_UNKNOWN\n",
      "auditweights_userOwner_USER_DELETE_MESSAGE\n",
      "auditweights_userOwner_USER_FEED_REMOVE\n",
      "auditweights_userOwner_USER_FORUM_MESSAGE_CREATE\n",
      "auditweights_userOwner_USER_INTERNAL_LIKE\n",
      "auditweights_userOwner_USER_INTERNAL_UNLIKE\n",
      "auditweights_userOwner_USER_PRESENT_SEND\n",
      "auditweights_userOwner_USER_PROFILE_VIEW\n",
      "auditweights_userOwner_USER_SEND_MESSAGE\n",
      "auditweights_userOwner_USER_STATUS_COMMENT_CREATE\n",
      "auditweights_userOwner_VIDEO\n",
      "auditweights_userOwner_VOTE_POLL\n",
      "auditweights_x_ActorsRelations\n",
      "auditweights_likersSvd_spark_hyper\n",
      "auditweights_source_PROMO\n",
      "date\n"
     ]
    }
   ],
   "source": [
    "// посмотрим на список колонок\n",
    "for (col <- df.columns){\n",
    "    println(col)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------+\n",
      "| min(date)| max(date)|datediff(max(date), min(date))|\n",
      "+----------+----------+------------------------------+\n",
      "|2018-02-01|2018-03-21|                            48|\n",
      "+----------+----------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// посмотрим минимальное, максимальное значение даты и разницу между ними\n",
    "spark.sql(\"\"\"\n",
    "    select min(date), max(date), datediff(max(date), min(date))\n",
    "    from trainDF\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### будем считать, что если пользователь не совершал никаких действий более трех недель, то он уже и не вернется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_tgt\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, instanceId_objectType: string ... 168 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_tgt = spark.sql(\"\"\"\n",
    "with cte_last_action as (\n",
    "    select instanceId_userId, max(date) as md from trainDF group by instanceId_userId\n",
    ")\n",
    ", cte_target as (\n",
    "    select \n",
    "        instanceId_userId\n",
    "        , case when datediff('2018-03-21', md) > 21 then 1 else 0 end as target\n",
    "    from cte_last_action\n",
    ") select e.*, t.target from trainDF e\n",
    "    join cte_target t on e.instanceId_userId = t.instanceId_userId\n",
    "\"\"\")\n",
    "\n",
    "df_tgt.createOrReplaceTempView(\"df_tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|target|count(1)|\n",
      "+------+--------+\n",
      "|     1| 1925038|\n",
      "|     0|16361537|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// посмотрим сколько у нас получилось оттёкших пользователей\n",
    "spark.sql(\"\"\"\n",
    "    select target, count(*) from df_tgt group by target\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовить признаки, обучить модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдвинем гипотезу, что отток напрямую зависит либо от общего кол-ва взаимодействий с пабликами в течение последних Х недель, или же от каких-то конкретных взаимодействий в течение последних Х недель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_exp_tgt\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, fb_exp: string ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// расширим наш врейм, вытащив список реакций в каждую отдельную строку\n",
    "\n",
    "var df_exp_tgt = spark.sql(\"\"\"\n",
    "    select \n",
    "        instanceId_userId\n",
    "        , explode(feedback) as fb_exp \n",
    "        , date\n",
    "        , target\n",
    "    from df_tgt t \n",
    "\"\"\")\n",
    "df_exp_tgt.createOrReplaceTempView(\"df_exp_tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_bucket\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, fb_exp: string ... 2 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// разобьем разницу между датой действия и последней даты на группы с шагом 7\n",
    "// , чтобы на выходе можно было посчитать кол-во действий в ретроспективе\n",
    "// , сгруппировав их по этим \"лагам времени\" и типам реакций\n",
    "\n",
    "val df_bucket = spark.sql(\"\"\"\n",
    "    select \n",
    "        instanceId_userId\n",
    "        , explode(feedback) as fb_exp \n",
    "        , floor(datediff('2018-03-21',date) / 7) date_diff_bucket\n",
    "        , target\n",
    "    from df_tgt t \n",
    "    order by date\n",
    "\"\"\")\n",
    "df_bucket.createOrReplaceTempView(\"df_bucket\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_act_type_lag\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, act_type: string ... 1 more field]\n",
       "\u001b[36mdf_pivot_act_type\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 62 more fields]\n",
       "\u001b[36mdf_lag\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, total_lag: string ... 1 more field]\n",
       "\u001b[36mdf_pivot_lag\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, total_0: bigint ... 6 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// сделаем фрейм с разбивкой по типу действия + недельному лагу\n",
    "val df_act_type_lag = spark.sql(\"\"\"\n",
    "select \n",
    "    instanceId_userId\n",
    "    , concat(fb_exp, '_', date_diff_bucket) as act_type\n",
    "    , target\n",
    "from df_bucket\n",
    "\"\"\")\n",
    "\n",
    "val df_pivot_act_type = df_act_type_lag\n",
    "    .groupBy(\"instanceId_userId\")\n",
    "    .pivot(\"act_type\")\n",
    "    .count()\n",
    "    .na.fill(0)\n",
    "\n",
    "df_pivot_act_type.createOrReplaceTempView(\"df_pivot_act_type\")\n",
    "\n",
    "// сделаем фрейм с разбивкой по недельному лагу\n",
    "val df_lag = spark.sql(\"\"\"\n",
    "select \n",
    "    instanceId_userId\n",
    "    , concat('total_', date_diff_bucket) as total_lag\n",
    "    , target\n",
    "from df_bucket\n",
    "\"\"\")\n",
    "\n",
    "val df_pivot_lag = df_lag\n",
    "    .groupBy(\"instanceId_userId\")\n",
    "    .pivot(\"total_lag\")\n",
    "    .count()\n",
    "    .na.fill(0)\n",
    "\n",
    "df_pivot_lag.createOrReplaceTempView(\"df_pivot_lag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_for_split\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 70 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_for_split = spark.sql(\"\"\"\n",
    "    select \n",
    "        pa.*\n",
    "        , la.total_0\n",
    "        , la.total_1\n",
    "        , la.total_2\n",
    "        , la.total_3\n",
    "        , la.total_4\n",
    "        , la.total_5\n",
    "        , la.total_6\n",
    "        , t.target\n",
    "    from df_tgt t\n",
    "    left join df_pivot_act_type pa on t.instanceId_userId = pa.instanceId_userId\n",
    "    left join df_pivot_lag la on t.instanceId_userId = la.instanceId_userId\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, получили разряженную матрицу с кол-ом реакций, разбитых по неделям и по типам + неделям. Теперь выборку можно разбить на трейн и тест, обучить модель и посмотреть что из этого вышло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.VectorAssembler\n",
       "\n",
       "// получим массив колонок с фичами\n",
       "\u001b[39m\n",
       "\u001b[36mf_arr\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"Clicked_1\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_2\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_3\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_4\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_5\"\u001b[39m,\n",
       "  \u001b[32m\"Clicked_6\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_1\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_2\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_3\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_4\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_5\"\u001b[39m,\n",
       "  \u001b[32m\"Commented_6\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_1\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_2\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_3\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_4\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_5\"\u001b[39m,\n",
       "  \u001b[32m\"Complaint_6\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_1\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_2\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_3\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_4\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_5\"\u001b[39m,\n",
       "  \u001b[32m\"Disliked_6\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_1\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_2\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_3\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_4\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_5\"\u001b[39m,\n",
       "  \u001b[32m\"Ignored_6\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_1\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_2\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_3\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_4\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_5\"\u001b[39m,\n",
       "  \u001b[32m\"Liked_6\"\u001b[39m,\n",
       "  \u001b[32m\"ReShared_1\"\u001b[39m,\n",
       "  \u001b[32m\"ReShared_2\"\u001b[39m,\n",
       "...\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_3f6770eb7024\n",
       "\u001b[36mdf_features\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 71 more fields]\n",
       "\u001b[36mtrainDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [instanceId_userId: int, Clicked_0: bigint ... 71 more fields]\n",
       "\u001b[36mtestDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [instanceId_userId: int, Clicked_0: bigint ... 71 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// для начала нам необходимо объединить все фичи в один вектор, чтобы потом скормить его модели\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "// получим массив колонок с фичами\n",
    "var f_arr = Array[String]()\n",
    "for (col <- df_for_split.columns){\n",
    "    if(col != \"instanceId_userId\" && col != \"target\" && col.endsWith(\"_0\") != true)\n",
    "        // срезали последнюю неделю, чтобы моделе было интереснее =)\n",
    "        f_arr = f_arr :+ col\n",
    "}\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(f_arr)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val df_features = assembler.transform(df_for_split)\n",
    "\n",
    "// разделим набор данных на трейн и тест\n",
    "val Array(trainDF, testDF) = df_features.randomSplit(Array(0.7, 0.3), seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "// Train a GBT model.\n",
       "\u001b[39m\n",
       "\u001b[36mgbt\u001b[39m: \u001b[32mGBTClassifier\u001b[39m = gbtc_cec363ded67e\n",
       "\u001b[36mgbtModel\u001b[39m: \u001b[32mGBTClassificationModel\u001b[39m = GBTClassificationModel (uid=gbtc_cec363ded67e) with 10 trees"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Попробуем обучить модель градиентного бустинга\n",
    "\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"target\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setMaxIter(10)\n",
    "\n",
    "val gbtModel = gbt.fit(trainDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|target|prediction|count(1)|\n",
      "+------+----------+--------+\n",
      "|     1|       0.0|   97997|\n",
      "|     0|       0.0| 4651170|\n",
      "|     1|       1.0|  480112|\n",
      "|     0|       1.0|  255063|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_gbt_pred\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 74 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_gbt_pred = gbtModel.transform(testDF)\n",
    "df_gbt_pred.createOrReplaceTempView(\"df_gbt_pred\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select target, prediction, count(*)\n",
    "    from df_gbt_pred\n",
    "    group by target, prediction\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mTP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m480112.0\u001b[39m\n",
       "\u001b[36mTN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m4651170.0\u001b[39m\n",
       "\u001b[36mFP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m255063.0\u001b[39m\n",
       "\u001b[36mFN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m97997.0\u001b[39m\n",
       "\u001b[36mPPV\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.6530581154146972\u001b[39m\n",
       "\u001b[36mTPR\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.83048698428843\u001b[39m\n",
       "\u001b[36mF1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.7311624903676585\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// посчитаем F1Score для модели бустинга\n",
    "val TP = 480112.0\n",
    "val TN = 4651170.0\n",
    "val FP = 255063.0\n",
    "val FN = 97997.0\n",
    "\n",
    "val PPV = TP / (TP + FP) // Precision (Positive Predictive Value)\n",
    "val TPR = TP / (TP + FN) // Recall (True Positive Rate)\n",
    "val F1 = (2 * PPV * TPR) / (PPV + TPR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.NaiveBayes\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mnbModel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mNaiveBayesModel\u001b[39m = NaiveBayesModel (uid=nb_09faf60a372e) with 2 classes"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Попробуем обучить модель наивного байеса\n",
    "\n",
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "\n",
    "val nbModel = new NaiveBayes()\n",
    "    .setLabelCol(\"target\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|target|prediction|count(1)|\n",
      "+------+----------+--------+\n",
      "|     1|       0.0|  242299|\n",
      "|     0|       0.0| 4670866|\n",
      "|     1|       1.0|  335810|\n",
      "|     0|       1.0|  235367|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_nb_pred\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, Clicked_0: bigint ... 74 more fields]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_nb_pred = nbModel.transform(testDF)\n",
    "df_nb_pred.createOrReplaceTempView(\"df_nb_pred\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select target, prediction, count(*)\n",
    "    from df_nb_pred\n",
    "    group by target, prediction\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mTP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m335810.0\u001b[39m\n",
       "\u001b[36mTN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m4670866.0\u001b[39m\n",
       "\u001b[36mFP\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m255063.0\u001b[39m\n",
       "\u001b[36mFN\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m235367.0\u001b[39m\n",
       "\u001b[36mPPV\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.5683285579134603\u001b[39m\n",
       "\u001b[36mTPR\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.587926334568794\u001b[39m\n",
       "\u001b[36mF1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.5779613613872037\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// посчитаем F1Score для модели бустинга\n",
    "val TP = 335810.0\n",
    "val TN = 4670866.0\n",
    "val FP = 255063.0\n",
    "val FN = 235367.0\n",
    "\n",
    "val PPV = TP / (TP + FP) // Precision (Positive Predictive Value)\n",
    "val TPR = TP / (TP + FN) // Recall (True Positive Rate)\n",
    "val F1 = (2 * PPV * TPR) / (PPV + TPR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показазатели конечно не фонтан, но будем считать, что нас это устраивает т.к. по заданию перед нами не стоит цель получить максимально точную модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Интерпретируем модель, сделать сегментацию пользователей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь вспоминаем, что с интерпретацией моделей в скале всё очень плохо, страдаем какое-то время. Вспоминаем, что есть PravdaML и пробуем запилить модель, используюя PravdaML, чтобы можно было интрпретировать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "// import org.apache.spark.ml.odkl.MatrixLBFGS\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.odkl.XGBoostClassifier\u001b[39m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`ru.odnoklassniki::pravda-ml:0.6.2`\n",
    "// import org.apache.spark.ml.odkl.MatrixLBFGS\n",
    "import org.apache.spark.ml.classification.odkl.XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *(24) HashAggregate(keys=[], functions=[partial_max(target#1440221)], output=[max#1440299])\n   +- *(24) Project [cast(target#363 as double) AS target#1440221]\n      +- *(24) Sample 0.0, 0.7, false, 42\n         +- *(24) Project [target#363]\n            +- *(24) Sort [instanceId_userId#2071 ASC NULLS FIRST, Clicked_0#2072L ASC NULLS FIRST, Clicked_1#2073L ASC NULLS FIRST, Clicked_2#2074L ASC NULLS FIRST, Clicked_3#2075L ASC NULLS FIRST, Clicked_4#2076L ASC NULLS FIRST, Clicked_5#2077L ASC NULLS FIRST, Clicked_6#2078L ASC NULLS FIRST, Commented_0#2079L ASC NULLS FIRST, Commented_1#2080L ASC NULLS FIRST, Commented_2#2081L ASC NULLS FIRST, Commented_3#2082L ASC NULLS FIRST, Commented_4#2083L ASC NULLS FIRST, Commented_5#2084L ASC NULLS FIRST, Commented_6#2085L ASC NULLS FIRST, Complaint_0#2086L ASC NULLS FIRST, Complaint_1#2087L ASC NULLS FIRST, Complaint_2#2088L ASC NULLS FIRST, Complaint_3#2089L ASC NULLS FIRST, Complaint_4#2090L ASC NULLS FIRST, Complaint_5#2091L ASC NULLS FIRST, Complaint_6#2092L ASC NULLS FIRST, Disliked_0#2093L ASC NULLS FIRST, Disliked_1#2094L ASC NULLS FIRST, ... 49 more fields], false, 0\n               +- *(24) Project [instanceId_userId#2071, Clicked_0#2072L, Clicked_1#2073L, Clicked_2#2074L, Clicked_3#2075L, Clicked_4#2076L, Clicked_5#2077L, Clicked_6#2078L, Commented_0#2079L, Commented_1#2080L, Commented_2#2081L, Commented_3#2082L, Commented_4#2083L, Commented_5#2084L, Commented_6#2085L, Complaint_0#2086L, Complaint_1#2087L, Complaint_2#2088L, Complaint_3#2089L, Complaint_4#2090L, Complaint_5#2091L, Complaint_6#2092L, Disliked_0#2093L, Disliked_1#2094L, ... 49 more fields]\n                  +- SortMergeJoin [instanceId_userId#0], [instanceId_userId#3258], LeftOuter\n                     :- SortMergeJoin [instanceId_userId#0], [instanceId_userId#2071], LeftOuter\n                     :  :- *(5) Project [instanceId_userId#0, target#363]\n                     :  :  +- *(5) SortMergeJoin [instanceId_userId#0], [instanceId_userId#365], Inner\n                     :  :     :- *(2) Sort [instanceId_userId#0 ASC NULLS FIRST], false, 0\n                     :  :     :  +- Exchange hashpartitioning(instanceId_userId#0, 200)\n                     :  :     :     +- *(1) Project [instanceId_userId#0]\n                     :  :     :        +- *(1) Filter isnotnull(instanceId_userId#0)\n                     :  :     :           +- *(1) FileScan parquet [instanceId_userId#0,date#168] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/train], PartitionCount: 48, PartitionFilters: [], PushedFilters: [IsNotNull(instanceId_userId)], ReadSchema: struct<instanceId_userId:int>\n                     :  :     +- *(4) Sort [instanceId_userId#365 ASC NULLS FIRST], false, 0\n                     :  :        +- *(4) HashAggregate(keys=[instanceId_userId#365], functions=[max(date#533)], output=[instanceId_userId#365, target#363])\n                     :  :           +- Exchange hashpartitioning(instanceId_userId#365, 200)\n                     :  :              +- *(3) HashAggregate(keys=[instanceId_userId#365], functions=[partial_max(date#533)], output=[instanceId_userId#365, max#717])\n                     :  :                 +- *(3) Project [instanceId_userId#365, date#533]\n                     :  :                    +- *(3) Filter isnotnull(instanceId_userId#365)\n                     :  :                       +- *(3) FileScan parquet [instanceId_userId#365,date#533] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/train], PartitionCount: 48, PartitionFilters: [], PushedFilters: [IsNotNull(instanceId_userId)], ReadSchema: struct<instanceId_userId:int>\n                     :  +- *(14) Sort [instanceId_userId#2071 ASC NULLS FIRST], false, 0\n                     :     +- Exchange hashpartitioning(instanceId_userId#2071, 200)\n                     :        +- HashAggregate(keys=[instanceId_userId#0], functions=[pivotfirst(act_type#732, count(1) AS `count`#1751L, Clicked_0, Clicked_1, Clicked_2, Clicked_3, Clicked_4, Clicked_5, Clicked_6, Commented_0, Commented_1, Commented_2, Commented_3, Commented_4, Commented_5, Commented_6, Complaint_0, Complaint_1, Complaint_2, Complaint_3, Complaint_4, Complaint_5, Complaint_6, Disliked_0, Disliked_1, Disliked_2, Disliked_3, Disliked_4, Disliked_5, Disliked_6, Ignored_0, Ignored_1, Ignored_2, Ignored_3, Ignored_4, Ignored_5, Ignored_6, Liked_0, Liked_1, Liked_2, Liked_3, Liked_4, Liked_5, Liked_6, ReShared_0, ReShared_1, ReShared_2, ReShared_3, ReShared_4, ReShared_5, ReShared_6, Unliked_0, Unliked_1, Unliked_2, Unliked_3, Unliked_4, Unliked_5, Unliked_6, Viewed_0, Viewed_1, Viewed_2, Viewed_3, Viewed_4, Viewed_5, Viewed_6, 0, 0)], output=[instanceId_userId#2071, Clicked_0#2072L, Clicked_1#2073L, Clicked_2#2074L, Clicked_3#2075L, Clicked_4#2076L, Clicked_5#2077L, Clicked_6#2078L, Commented_0#2079L, Commented_1#2080L, Commented_2#2081L, Commented_3#2082L, Commented_4#2083L, Commented_5#2084L, Commented_6#2085L, Complaint_0#2086L, Complaint_1#2087L, Complaint_2#2088L, Complaint_3#2089L, Complaint_4#2090L, Complaint_5#2091L, Complaint_6#2092L, Disliked_0#2093L, Disliked_1#2094L, ... 40 more fields])\n                     :           +- Exchange hashpartitioning(instanceId_userId#0, 200)\n                     :              +- HashAggregate(keys=[instanceId_userId#0], functions=[partial_pivotfirst(act_type#732, count(1) AS `count`#1751L, Clicked_0, Clicked_1, Clicked_2, Clicked_3, Clicked_4, Clicked_5, Clicked_6, Commented_0, Commented_1, Commented_2, Commented_3, Commented_4, Commented_5, Commented_6, Complaint_0, Complaint_1, Complaint_2, Complaint_3, Complaint_4, Complaint_5, Complaint_6, Disliked_0, Disliked_1, Disliked_2, Disliked_3, Disliked_4, Disliked_5, Disliked_6, Ignored_0, Ignored_1, Ignored_2, Ignored_3, Ignored_4, Ignored_5, Ignored_6, Liked_0, Liked_1, Liked_2, Liked_3, Liked_4, Liked_5, Liked_6, ReShared_0, ReShared_1, ReShared_2, ReShared_3, ReShared_4, ReShared_5, ReShared_6, Unliked_0, Unliked_1, Unliked_2, Unliked_3, Unliked_4, Unliked_5, Unliked_6, Viewed_0, Viewed_1, Viewed_2, Viewed_3, Viewed_4, Viewed_5, Viewed_6, 0, 0)], output=[instanceId_userId#0, Clicked_0#1815L, Clicked_1#1816L, Clicked_2#1817L, Clicked_3#1818L, Clicked_4#1819L, Clicked_5#1820L, Clicked_6#1821L, Commented_0#1822L, Commented_1#1823L, Commented_2#1824L, Commented_3#1825L, Commented_4#1826L, Commented_5#1827L, Commented_6#1828L, Complaint_0#1829L, Complaint_1#1830L, Complaint_2#1831L, Complaint_3#1832L, Complaint_4#1833L, Complaint_5#1834L, Complaint_6#1835L, Disliked_0#1836L, Disliked_1#1837L, ... 40 more fields])\n                     :                 +- *(13) HashAggregate(keys=[instanceId_userId#0, act_type#732], functions=[count(1)], output=[instanceId_userId#0, act_type#732, count(1) AS `count`#1751L])\n                     :                    +- Exchange hashpartitioning(instanceId_userId#0, act_type#732, 200)\n                     :                       +- *(12) HashAggregate(keys=[instanceId_userId#0, act_type#732], functions=[partial_count(1)], output=[instanceId_userId#0, act_type#732, count#3695L])\n                     :                          +- *(12) Project [instanceId_userId#0, concat(fb_exp#727, _, cast(date_diff_bucket#726L as string)) AS act_type#732]\n                     :                             +- *(12) Sort [date#168 ASC NULLS FIRST], true, 0\n                     :                                +- Exchange rangepartitioning(date#168 ASC NULLS FIRST, 200)\n                     :                                   +- *(11) Project [instanceId_userId#0, fb_exp#727, FLOOR((cast(datediff(17611, date#168) as double) / 7.0)) AS date_diff_bucket#726L, date#168]\n                     :                                      +- Generate explode(feedback#108), [instanceId_userId#0, date#168], false, [fb_exp#727]\n                     :                                         +- *(10) Project [instanceId_userId#0, feedback#108, date#168]\n                     :                                            +- *(10) SortMergeJoin [instanceId_userId#0], [instanceId_userId#365], Inner\n                     :                                               :- *(7) Sort [instanceId_userId#0 ASC NULLS FIRST], false, 0\n                     :                                               :  +- Exchange hashpartitioning(instanceId_userId#0, 200)\n                     :                                               :     +- *(6) Project [instanceId_userId#0, feedback#108, date#168]\n                     :                                               :        +- *(6) Filter isnotnull(instanceId_userId#0)\n                     :                                               :           +- *(6) FileScan parquet [instanceId_userId#0,feedback#108,date#168] Batched: false, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/train], PartitionCount: 48, PartitionFilters: [], PushedFilters: [IsNotNull(instanceId_userId)], ReadSchema: struct<instanceId_userId:int,feedback:array<string>>\n                     :                                               +- *(9) Sort [instanceId_userId#365 ASC NULLS FIRST], false, 0\n                     :                                                  +- *(9) HashAggregate(keys=[instanceId_userId#365], functions=[], output=[instanceId_userId#365])\n                     :                                                     +- Exchange hashpartitioning(instanceId_userId#365, 200)\n                     :                                                        +- *(8) HashAggregate(keys=[instanceId_userId#365], functions=[], output=[instanceId_userId#365])\n                     :                                                           +- *(8) Project [instanceId_userId#365]\n                     :                                                              +- *(8) Filter isnotnull(instanceId_userId#365)\n                     :                                                                 +- *(8) FileScan parquet [instanceId_userId#365,date#533] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/train], PartitionCount: 48, PartitionFilters: [], PushedFilters: [IsNotNull(instanceId_userId)], ReadSchema: struct<instanceId_userId:int>\n                     +- *(23) Sort [instanceId_userId#3258 ASC NULLS FIRST], false, 0\n                        +- Exchange hashpartitioning(instanceId_userId#3258, 200)\n                           +- HashAggregate(keys=[instanceId_userId#0], functions=[pivotfirst(total_lag#2199, count(1) AS `count`#3218L, total_0, total_1, total_2, total_3, total_4, total_5, total_6, 0, 0)], output=[instanceId_userId#3258, total_0#3259L, total_1#3260L, total_2#3261L, total_3#3262L, total_4#3263L, total_5#3264L, total_6#3265L])\n                              +- Exchange hashpartitioning(instanceId_userId#0, 200)\n                                 +- HashAggregate(keys=[instanceId_userId#0], functions=[partial_pivotfirst(total_lag#2199, count(1) AS `count`#3218L, total_0, total_1, total_2, total_3, total_4, total_5, total_6, 0, 0)], output=[instanceId_userId#0, total_0#3226L, total_1#3227L, total_2#3228L, total_3#3229L, total_4#3230L, total_5#3231L, total_6#3232L])\n                                    +- *(22) HashAggregate(keys=[instanceId_userId#0, total_lag#2199], functions=[count(1)], output=[instanceId_userId#0, total_lag#2199, count(1) AS `count`#3218L])\n                                       +- Exchange hashpartitioning(instanceId_userId#0, total_lag#2199, 200)\n                                          +- *(21) HashAggregate(keys=[instanceId_userId#0, total_lag#2199], functions=[partial_count(1)], output=[instanceId_userId#0, total_lag#2199, count#3711L])\n                                             +- *(21) Project [instanceId_userId#0, concat(total_, cast(date_diff_bucket#726L as string)) AS total_lag#2199]\n                                                +- *(21) Sort [date#168 ASC NULLS FIRST], true, 0\n                                                   +- Exchange rangepartitioning(date#168 ASC NULLS FIRST, 200)\n                                                      +- *(20) Project [instanceId_userId#0, FLOOR((cast(datediff(17611, date#168) as double) / 7.0)) AS date_diff_bucket#726L, date#168]\n                                                         +- Generate explode(feedback#108), [instanceId_userId#0, date#168], false, [fb_exp#727]\n                                                            +- *(19) Project [instanceId_userId#0, feedback#108, date#168]\n                                                               +- *(19) SortMergeJoin [instanceId_userId#0], [instanceId_userId#365], Inner\n                                                                  :- *(16) Sort [instanceId_userId#0 ASC NULLS FIRST], false, 0\n                                                                  :  +- ReusedExchange [instanceId_userId#0, feedback#108, date#168], Exchange hashpartitioning(instanceId_userId#0, 200)\n                                                                  +- *(18) Sort [instanceId_userId#365 ASC NULLS FIRST], false, 0\n                                                                     +- *(18) HashAggregate(keys=[instanceId_userId#365], functions=[], output=[instanceId_userId#365])\n                                                                        +- ReusedExchange [instanceId_userId#365], Exchange hashpartitioning(instanceId_userId#365, 200)\n\u001b[39m\n  org.apache.spark.sql.catalyst.errors.package$.attachTree(\u001b[32mpackage.scala\u001b[39m:\u001b[32m56\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(\u001b[32mHashAggregateExec.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m247\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m339\u001b[39m)\n  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3389\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2764\u001b[39m)\n  org.apache.spark.ml.classification.Classifier.getNumClasses(\u001b[32mClassifier.scala\u001b[39m:\u001b[32m111\u001b[39m)\n  ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier.train(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m166\u001b[39m)\n  ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier.train(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m40\u001b[39m)\n  org.apache.spark.ml.Predictor.fit(\u001b[32mPredictor.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  org.apache.spark.ml.classification.odkl.XGBoostClassifier.trainInternal(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m154\u001b[39m)\n  org.apache.spark.ml.classification.odkl.XGBoostClassifier.fit(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m173\u001b[39m)\n  ammonite.$sess.cmd44$Helper.<init>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m3\u001b[39m)\n  ammonite.$sess.cmd44$.<init>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd44$.<clinit>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31morg.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(instanceId_userId#0, 200)\n+- *(1) Project [instanceId_userId#0]\n   +- *(1) Filter isnotnull(instanceId_userId#0)\n      +- *(1) FileScan parquet [instanceId_userId#0,date#168] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/train], PartitionCount: 48, PartitionFilters: [], PushedFilters: [IsNotNull(instanceId_userId)], ReadSchema: struct<instanceId_userId:int>\n\u001b[39m\n  org.apache.spark.sql.catalyst.errors.package$.attachTree(\u001b[32mpackage.scala\u001b[39m:\u001b[32m56\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.SortExec.inputRDDs(\u001b[32mSortExec.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m383\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(\u001b[32mSortMergeJoinExec.scala\u001b[39m:\u001b[32m386\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(\u001b[32mSortMergeJoinExec.scala\u001b[39m:\u001b[32m150\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(\u001b[32mSortMergeJoinExec.scala\u001b[39m:\u001b[32m150\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.SortExec.inputRDDs(\u001b[32mSortExec.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.SampleExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(\u001b[32mHashAggregateExec.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m92\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m128\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.catalyst.errors.package$.attachTree(\u001b[32mpackage.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(\u001b[32mHashAggregateExec.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m247\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m339\u001b[39m)\n  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3389\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2764\u001b[39m)\n  org.apache.spark.ml.classification.Classifier.getNumClasses(\u001b[32mClassifier.scala\u001b[39m:\u001b[32m111\u001b[39m)\n  ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier.train(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m166\u001b[39m)\n  ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier.train(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m40\u001b[39m)\n  org.apache.spark.ml.Predictor.fit(\u001b[32mPredictor.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  org.apache.spark.ml.classification.odkl.XGBoostClassifier.trainInternal(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m154\u001b[39m)\n  org.apache.spark.ml.classification.odkl.XGBoostClassifier.fit(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m173\u001b[39m)\n  ammonite.$sess.cmd44$Helper.<init>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m3\u001b[39m)\n  ammonite.$sess.cmd44$.<init>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd44$.<clinit>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\nammonite.$sess.cmd3$Helper.<init>(cmd3.sc:7)\nammonite.$sess.cmd3$.<init>(cmd3.sc:7)\nammonite.$sess.cmd3$.<clinit>(cmd3.sc)\nammonite.$sess.cmd3.$main(cmd3.sc)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\nammonite.runtime.Evaluator$$anon$1$$anonfun$evalMain$1.apply(Evaluator.scala:112)\nammonite.util.Util$.withContextClassloader(Util.scala:16)\nammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:94)\nammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1$$anonfun$apply$6.apply(Evaluator.scala:131)\nammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1$$anonfun$apply$6.apply(Evaluator.scala:125)\nammonite.util.Catching.map(Res.scala:117)\nammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1.apply(Evaluator.scala:125)\nammonite.runtime.Evaluator$$anon$1$$anonfun$processLine$1.apply(Evaluator.scala:124)\nammonite.util.Res$Success.flatMap(Res.scala:62)\nammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:124)\nammonite.interp.Interpreter$$anonfun$evaluateLine$2$$anonfun$apply$15.apply(Interpreter.scala:287)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \u001b[39m\n  org.apache.spark.SparkContext.assertNotStopped(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m100\u001b[39m)\n  org.apache.spark.SparkContext.broadcast(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m1486\u001b[39m)\n  org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(\u001b[32mParquetFileFormat.scala\u001b[39m:\u001b[32m328\u001b[39m)\n  org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(\u001b[32mDataSourceScanExec.scala\u001b[39m:\u001b[32m309\u001b[39m)\n  org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(\u001b[32mDataSourceScanExec.scala\u001b[39m:\u001b[32m305\u001b[39m)\n  org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(\u001b[32mDataSourceScanExec.scala\u001b[39m:\u001b[32m327\u001b[39m)\n  org.apache.spark.sql.execution.FilterExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m92\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m128\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.catalyst.errors.package$.attachTree(\u001b[32mpackage.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.SortExec.inputRDDs(\u001b[32mSortExec.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m383\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(\u001b[32mSortMergeJoinExec.scala\u001b[39m:\u001b[32m386\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(\u001b[32mSortMergeJoinExec.scala\u001b[39m:\u001b[32m150\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(\u001b[32mSortMergeJoinExec.scala\u001b[39m:\u001b[32m150\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.SortExec.inputRDDs(\u001b[32mSortExec.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.SampleExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.execution.ProjectExec.inputRDDs(\u001b[32mbasicPhysicalOperators.scala\u001b[39m:\u001b[32m41\u001b[39m)\n  org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(\u001b[32mHashAggregateExec.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m92\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m128\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.catalyst.errors.package$.attachTree(\u001b[32mpackage.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(\u001b[32mShuffleExchangeExec.scala\u001b[39m:\u001b[32m119\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.InputAdapter.inputRDDs(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m391\u001b[39m)\n  org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(\u001b[32mHashAggregateExec.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m627\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m247\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m339\u001b[39m)\n  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3389\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3370\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3369\u001b[39m)\n  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2550\u001b[39m)\n  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2764\u001b[39m)\n  org.apache.spark.ml.classification.Classifier.getNumClasses(\u001b[32mClassifier.scala\u001b[39m:\u001b[32m111\u001b[39m)\n  ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier.train(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m166\u001b[39m)\n  ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier.train(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m40\u001b[39m)\n  org.apache.spark.ml.Predictor.fit(\u001b[32mPredictor.scala\u001b[39m:\u001b[32m118\u001b[39m)\n  org.apache.spark.ml.classification.odkl.XGBoostClassifier.trainInternal(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m154\u001b[39m)\n  org.apache.spark.ml.classification.odkl.XGBoostClassifier.fit(\u001b[32mXGBoostClassifier.scala\u001b[39m:\u001b[32m173\u001b[39m)\n  ammonite.$sess.cmd44$Helper.<init>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m3\u001b[39m)\n  ammonite.$sess.cmd44$.<init>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd44$.<clinit>(\u001b[32mcmd44.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val pmlXGBModel = new XGBoostClassifier()\n",
    "    .setLabelCol(\"target\")\n",
    "    .fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres45\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@75eddd19"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
